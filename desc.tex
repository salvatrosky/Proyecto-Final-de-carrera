%-*- ES -*-
%----------------------------------------------------------------------
% Capitulo 7: Descripción del problema

En éste capítulo se verá por qué los protocolos http y smtp son inseguros, introducción al snnifin, spoofing, arp attack
%----------------------------------------------------------------------

\section{El problema de los protocolos http y smtp}


(Agregar una intro)

\subsection{Establishing Authority}

HTTP relies on the notion of an authoritative response: a response
that has been determined by (or at the direction of) the authority
identified within the target URI to be the most appropriate response
for that request given the state of the target resource at the time
of response message origination.  Providing a response from a
non-authoritative source, such as a shared cache, is often useful to
improve performance and availability, but only to the extent that the
source can be trusted or the distrusted response can be safely used.

Unfortunately, establishing authority can be difficult.  For example,
phishing is an attack on the user's perception of authority, where
that perception can be misled by presenting similar branding in
hypertext, possibly aided by userinfo obfuscating the authority
component (see Section 2.7.1).  User agents can reduce the impact of
phishing attacks by enabling users to easily inspect a target URI
prior to making an action, by prominently distinguishing (or
rejecting) userinfo when present, and by not sending stored
credentials and cookies when the referring document is from an
unknown or untrusted source.

When a registered name is used in the authority component, the "http"
URI scheme (Section 2.7.1) relies on the user's local name resolution
service to determine where it can find authoritative responses.  This
means that any attack on a user's network host table, cached names,
or name resolution libraries becomes an avenue for attack on
establishing authority.  Likewise, the user's choice of server for
Domain Name Service (DNS), and the hierarchy of servers from which it
obtains resolution results, could impact the authenticity of address
mappings; DNS Security Extensions are one way to
improve authenticity.

Furthermore, after an IP address is obtained, establishing authority
for an "http" URI is vulnerable to attacks on Internet Protocol
routing.


\subsection{Risks of Intermediaries}

By their very nature, HTTP intermediaries are men-in-the-middle and,
thus, represent an opportunity for man-in-the-middle attacks.
Compromise of the systems on which the intermediaries run can result
in serious security and privacy problems.  Intermediaries might have
access to security-related information, personal information about
individual users and organizations, and proprietary information
belonging to users and content providers.  A compromised
intermediary, or an intermediary implemented or configured without
regard to security and privacy considerations, might be used in the
commission of a wide range of potential attacks.

Intermediaries that contain a shared cache are especially vulnerable
to cache poisoning attacks.
Implementers need to consider the privacy and security implications
of their design and coding decisions, and of the configuration
options they provide to operators (especially the default
configuration).

Users need to be aware that intermediaries are no more trustworthy
than the people who run them; HTTP itself cannot solve this problem.

\subsection{Attacks via Protocol Element Length}

Because HTTP uses mostly textual, character-delimited fields, parsers
are often vulnerable to attacks based on sending very long (or very
slow) streams of data, particularly where an implementation is
expecting a protocol element with no predefined length.



\subsection{Response Splitting}

Response splitting (a.k.a, CRLF injection) is a common technique,
used in various attacks on Web usage, that exploits the line-based
nature of HTTP message framing and the ordered association of
requests to responses on persistent connections [Klein].  This
technique can be particularly damaging when the requests pass through
a shared cache.

Response splitting exploits a vulnerability in servers (usually
within an application server) where an attacker can send encoded data
within some parameter of the request that is later decoded and echoed
within any of the response header fields of the response.  If the
decoded data is crafted to look like the response has ended and a
subsequent response has begun, the response has been split and the
content within the apparent second response is controlled by the
attacker.  The attacker can then make any other request on the same
persistent connection and trick the recipients (including
intermediaries) into believing that the second half of the split is
an authoritative answer to the second request.

For example, a parameter within the request-target might be read by
an application server and reused within a redirect, resulting in the
same parameter being echoed in the Location header field of the
response.  If the parameter is decoded by the application and not
properly encoded when placed in the response field, the attacker can
send encoded CRLF octets and other content that will make the
application's single response look like two or more responses.


\subsection{Request Smuggling}

Request smuggling ([Linhart]) is a technique that exploits
differences in protocol parsing among various recipients to hide
additional requests (which might otherwise be blocked or disabled by
policy) within an apparently harmless request.  Like response
splitting, request smuggling can lead to a variety of attacks on HTTP
usage.


\subsection{Message Integrity}

HTTP does not define a specific mechanism for ensuring message
integrity, instead relying on the error-detection ability of
underlying transport protocols and the use of length or
chunk-delimited framing to detect completeness.  Additional integrity
mechanisms, such as hash functions or digital signatures applied to
the content, can be selectively added to messages via extensible
metadata header fields.  Historically, the lack of a single integrity
mechanism has been justified by the informal nature of most HTTP
communication.  However, the prevalence of HTTP as an information
access mechanism has resulted in its increasing use within
environments where verification of message integrity is crucial.

User agents are encouraged to implement configurable means for
detecting and reporting failures of message integrity such that those
means can be enabled within environments for which integrity is
necessary.  For example, a browser being used to view medical history
or drug interaction information needs to indicate to the user when
such information is detected by the protocol to be incomplete,
expired, or corrupted during transfer.  Such mechanisms might be
selectively enabled via user agent extensions or the presence of
message integrity metadata in a response.  At a minimum, user agents
ought to provide some indication that allows a user to distinguish
between a complete and incomplete response message (Section 3.4) when
such verification is desired.

\subsection{Message Confidentiality}

HTTP relies on underlying transport protocols to provide message
confidentiality when that is desired.  HTTP has been specifically
designed to be independent of the transport protocol, such that it
can be used over many different forms of encrypted connection, with
the selection of such transports being identified by the choice of
URI scheme or within user agent configuration.

\section{Network Security Attacks: basic concepts}
There are mainly two types of network attacks – passive attack and active attack

Passive: This type of attack happens when sensitive information is monitored and
analyzed, possibly compromising the security of enterprises and their customers.
In short, network intruder intercepts data traveling through the network.
• Active: This type of attack happens when information is modified, altered or
demolished entirely by a hacker. Here the interloper starts instructions to disturb
the network’s regular process.

So the motives behind passive attackers and active attackers are totally different.
Whereas the motive of passive attackers is simply to steal sensitive information and
to analyze the traffic to steal future messages, the motive of active attackers is to stop
normal communication between two legitimate entities.
\subsection{Passive Attacks}
Passive attackers aremainly interested in stealing sensitive information. This happens
without the knowledge of the victim. As such passive attacks are difficult to detect
and thereby secure the network. The following are some of the passive attacks that
are in existence [7].
\begin{itemize}
    \item Traffic Analysis: Attacker senses the communication path between the sender
    and receiver.
    \item Monitoring: Attacker can read the confidential data, but he cannot edit or modify
    the data.
    \item Eavesdropping: This type of attack occurs in the mobile ad-hoc network where
    basically the attacker finds out some secret or confidential information from
    communication.
\end{itemize}

\subsection{Active Attacks}
The active attacks happen in such a manner so as to notify the victims that their
systems have been compromised. As a result, the victim stops communication with
the other party. Some of the active attacks are as follows [8].
\begin{itemize}
    \item Modification: Some alterations in the routing route is performed by the malicious
    node. This results in causing the sender to send messages through the long route,
    which causes communication delay. This is an attack on integrity as shown in
    Fig. 2.
    \item Wormhole: This attack is also called a tunneling attack. A packet is received
    by an attacker at one point. He then tunnels it to another malicious node in the
    network. This causes a beginner to assume that he found the shortest path in the
    network as shown in Fig. 3.
    \item Fabrication: A malicious node generates a false routing message that causes the
    generation of incorrect information about the route between devices. This is an
    attack on authenticity as shown in Fig. 4.    
    \item Spoofing: A malicious node miss-present his identity so that the sender changes
    his topology as shown in Fig. 5.
    \item Denial of services: A malicious node sends a message to the node and consumes
    the bandwidth of the network as given in Fig.
    \item  Man-in-the-middle: Attack—Also called a hijacking attack, it is an attack where
    the attacker secretly alters and relays the communications between two legitimate
    parties without their knowledge. These parties in turn are unaware of the secret
    hacker consider that they are doing direct communication with each other [13].
    Figure 12 depicts this attack.
\end{itemize}

\section{Herramientas utilizadas}
    \subsection{Kali Linux}
    BackTrack is one of the most famous Linux distribution systems, as can be proven by
the number of downloads that reached more than four million as of BackTrack Linux
4.0 pre final.

Kali Linux Version 1.0 was released on March 12, 2013. Five days later, Version 1.0.1
was released, which fixed the USB keyboard issue. In those five days, Kali has been
downloaded more than 90,000 times.

    Kali Linux is security-focused Linux distribution based on Debian. It's a rebranded
    version of the famous Linux distribution known as Backtrack, which came with
    a huge repository of open source hacking tools for network, wireless, and web
    application penetration testing. Although Kali Linux contains most of the tools
    from Backtrack, the main aim of Kali Linux is to make it portable so that it could
    be installed on devices based on the ARM architectures such as tablets and
    Chromebook, which makes the tools available at your disposal with much ease.

    Using open source hacking tools comes with a major drawback: they contain a
whole lot of dependencies when installed on Linux and they need to be installed in
a predefined sequence. Moreover, authors of some tools have not released accurate
documentation, which makes our life difficult.

Kali Linux simplifies this process; it contains many tools preinstalled with all the
dependencies and is in ready to use condition so that you can pay more attention
for the actual attack and not on installing the tool. Updates for tools installed in Kali
Linux are more frequently released, which helps you to keep the tools up to date. A
non-commercial toolkit that has all the major hacking tools preinstalled to test realworld networks and applications is a dream of every ethical hacker and the authors
of Kali Linux make every effort to make our life easy, which enables us to spend
more time on finding the actual flaws rather than building a toolkit.

(EXPLICAR COMO Y DONDE LO INSTALE)
(DECIR QUE FUE EL SISTEMA OPERATIVO UTILIZADO QUE CONTIENEN LAS SIGUIENTES HERRAMIENTAS)
\subsection{Wireshark}
Wireshark is one of the most popular, free, and open source network protocol
analyzers. Wireshark is preinstalled in Kali and ideal for network troubleshooting,
analysis, and for this chapter, a perfect tool to monitor traffic from potential targets
with the goal of capturing session tokens. Wireshark uses a GTK+ widget toolkit to
implement its user interface and pcap to capture packets. It operates very similarly to
a tcpdump command; however, acting as a graphical frontend with integrated sorting
and filtering options.
(HAY MAS, CON GRAFICOS EN)
Joseph Muniz, Aamir Lakhani - Web Penetration Testing with Kali Linux-Packt Publishing (2013)

    \subsection{Ettrcap}
    Ettercap is a free and open source comprehensive suite for man-in-the-middle-based
attacks.
Ettercap can be used for computer network protocol analysis and security auditing,
featuring sniffing live connections, content filtering, and support for active and passive
dissection of multiple protocols. Ettercap works by putting the attacker's network
interface into promiscuous mode and ARP for poisoning the victim machines.
\section{Virtualización}
(AGREGAR GRAFICOS SI O SI)
Virtualization provides abstraction on top of the actual resources we want to virtualize.
 The level at which this abstraction is applied changes the way that different
  virtualization techniques look.At a higher level, there are two major
   virtualization techniques based on the level of abstraction.
   • Virtual machine (VM)-based
   • Container-based
   Apart from these two virtualizing techniques, there are other techniques, such as unikernels, which are lightweight single-purpose VMs. IBM is currently attempting to run unikernels as processes with projects like Nabla. In this book, we will mainly look at VM-based and container-based virtualizations only.
\subsection{Máquinas virtuales}
VM-Based  VirtualizationThe VM-based approach virtualizes the complete OS. The abstraction
 it presents to the VM are virtual devices like virtual disks, virtual CPUs, and virtual 
 NICs. In other words, we can state that this is virtualizing the complete ISA
  (instruction set architecture); as an example, the x86 ISA.With virtual machines,
   multiple OSes can share the same hardware resources, with virtualized representations 
   of each of the resources available to the VM. For example, the OS on the virtual
    machine (also called the guest) can continue to do I/O operations on a disk 
    (in this case, it’s a virtual disk), thinking that it’s the only OS running on the 
    physical hardware (also called the host), although in actuality, it is shared by 
    multiple virtual machines as well as by the host OS.VMs are very similar to other 
    processes in the host OS. VMs execute in a hardware-isolated virtual address space 
    and at a lower privilege level than the host OS. The primary difference between a
     process and a VM is the ABI (Application Binary Interface) exposed by the host
      to the VM. In the case of a process, the exposed ABI has constructs like network 
      sockets, FDs, and so on, whereas with a full-fledged OS virtualization, the ABI 
      will have a virtual disk, a virtual CPU, virtual network cards, and so on.

Hypervisors
A special piece of software is used to virtualize the OS, called the hypervisor. The 
hypervisor itself has two parts:
• Virtual Machine Monitor (VMM): Used for trapping and emulating the privileged 
instruction set (which only the kernel of the operating system can perform).
The VMM has to satisfy three properties (Popek and Goldberg, 1973):
    • Isolation :  
Should isolate guests (VMs) from each other.
    • Equivalency : Should behave the same, 
with or without virtualization. This means we run the majority (almost all) of the 
instructions on the physical hardware without any translation, and so on.
    • Performance : 
Should perform as good as it does without any virtualization. This again means that 
the overhead of running a VM is minimal.

Device  Model
The device model of the hypervisor handles the I/O virtualization again by trapping 
and emulating and then delivering interrupts back to the specific virtual machine.
The device model handles:
•Memory  Virtualization
•Shadow Page Table
•CPU  Virtualization
•IO  Virtualization

\subsection{Container-Based  Virtualization}
Container-Based  VirtualizationThis form of virtualization doesn’t abstract the 
hardware but uses techniques within the Linux kernel to isolate access paths for
 different resources. It carves out a logical boundary within the same operating 
 system. As an example, we get a separate root file system, a separate process tree, 
 a separate network subsystem, and so on.


The Linux kernel is made up of several components and functionalities; the ones related to
containers are as follows:
Control groups (cgroups)
Namespaces
Security-Enhanced Linux (SELinux)
Cgroups
The cgroup functionality allows for limiting and prioritizing resources, such as CPUs,
RAM, the network, the filesystem, and so on. The main goal is to not exceed the
resources—to avoid wasting resources that might be needed for other processes.
Namespaces
The namespace functionality allows for partitioning of kernel resources, such that one set of
processes sees one set of resources, while another set of processes sees a different set of
resources. The feature works by having the same namespace for these resources in the
various sets of processes, but having those names refer to distinct resources. Examples of
resource names that can exist in multiple spaces (so that the named resources will be
partitioned) are process IDs, hostnames, user IDs, filenames, and some names associated
with network access and inter-process communication.
When a Linux system boots; that is, only one namespace is created. Processes and resources
will join the same namespace, until a different namespace is created, resources assigned to
it, and processes join it.
SELinux
SELinux is a module of the Linux kernel that provides a mechanism to enforce the security
of the system, with specific policies.
Basically, SELinux can limit programs from accessing files and network resources. The idea
is to limit the privileges of programs and daemons to a minimum, so that it can limit the
risk of system halt.
The preceding functionalities have been around for many years. Namespaces were first
released in 2002, and cgroups in 2005, by Google (cgroups were first named process
containers, and then cgroups). For example, SunSolaris 5.10, released at the beginning of
2005, provided support for Solaris containers.
Nowadays, Linux containers are the new buzzword, and some people think they are a
new means of virtualization.
Containerization uses resources directly, and does not need an emulator at all; the fewer
resources, the more efficiency. Different applications can run on the same host: isolated at
the kernel level and isolated by namespaces and cgroups. The kernel (that is, the OS) is
shared by all containers, as shown in the following diagram:
Containers
When we talk about containers, we are indirectly referring to two main concepts—a
container image and a running container image.
A container image is the definition of the container, wherein all software stacks are
installed as additional layers, as depicted by the following diagram:
A container image is typically made up of multiple layers.
The first layer is given by the base image, which provides the OS core functionalities, with
all of the tools needed to get started. Teams often work by building their own layers on
these base images. Users can also build on more advanced application images, which not
only have an OS, but which also include language runtimes, debugging tools, and libraries,
as shown in the following diagram:
Base images are built from the same utilities and libraries that are included in an OS. A
good base image provides a secure and stable platform on which to build applications. Red
Hat provides base images for Red Hat Enterprise Linux. These images can be used like a
normal OS. Users can customize them for their applications as necessary, installing
packages and enabling services to start up just like a normal Red Hat Enterprise Linux
Server.
Containers provide isolation by taking advantage of kernel technologies, like cgroups,
kernel namespaces, and SELinux, which have been battle-tested and used for years at
Google and the US Department of Defense, in order to provide application isolation.
Since containers use a shared kernel and container host, they reduce the amount of
resources required for the container itself, and are more lightweight when compared to
VMs. Therefore, containers provide an unmatched agility that is not feasible with VMs; for
example, it only takes a few seconds to start a new container. Furthermore, containers
support a more flexible model when it comes to CPU utilization and memory resources,
and allow for resource burst modes, so that applications can consume more resources when
required, within the defined boundaries.

\subsection{Docker}
Docker
Docker is an open-source engine that automates the deployment of
applications into containers. It was written by the team at Docker, Inc
(formerly dotCloud Inc, an early player in the Platform-as-a-Service
(PAAS) market), and released by them under the Apache 2.0 license.
Docker adds an application deployment
engine on top of a virtualized container execution environment. It is
designed to provide a lightweight and fast environment in which to run your
code as well as an efficient workflow to get that code from your laptop to
your test environment and then into production. Docker is incredibly
simple. Indeed, you can get started with Docker on a minimal host running
nothing but a compatible Linux kernel and a Docker binary. 

With Docker, Developers care about their applications running inside
containers, and Operations cares about managing the containers. Docker is
designed to enhance consistency by ensuring the environment in which
your developers write code matches the environments into which your
applications are deployed. This reduces the risk of “worked in dev, now an
ops problem.
\subsubsection{Docker components}
Let’s look at the core components that compose the Docker Community
Edition:
The Docker client and server, also called the Docker Engine.
Docker Images
Registries
Docker Containers
\paragraph{Docker client and server}
Docker is a client-server application. The Docker client talks to the Docker
server or daemon, which, in turn, does all the work. You’ll also sometimes
see the Docker daemon called the Docker Engine. Docker ships with a
command line client binary, docker, as well as a full RESTful API to
interact with the daemon: dockerd. You can run the Docker daemon and
client on the same host or connect your local Docker client to a remote
daemon running on another host. You can see Docker’s architecture
depicted here:
\paragraph{Docker images}
Images are the building blocks of the Docker world. You launch your
containers from images. Images are the “build” part of Docker’s life cycle.
They have a layered format, using Union file systems, that are built step-bystep using a series of instructions. For example:
Add a file.
Run a command.
Open a port.
You can consider images to be the “source code” for your containers. They
are highly portable and can be shared, stored, and updated. In the book,
we’ll learn how to use existing images as well as build our own images.
\paragraph{Registries}
Docker stores the images you build in registries. There are two types of
registries: public and private. Docker, Inc., operates the public registry for
images, called the Docker Hub. You can create an account on the Docker
Hub and use it to share and store your own images.
The Docker Hub also contains, at last count, over 10,000 images that other
people have built and shared. Want a Docker image for an Nginx web
server, the Asterisk open source PABX system, or a MySQL database? All
of these are available, along with a whole lot more.
You can also store images that you want to keep private on the Docker Hub.
These images might include source code or other proprietary information
you want to keep secure or only share with other members of your team or
organization.
You can also run your own private registry, and we’ll show you how to do
that in Chapter 4. This allows you to store images behind your firewall,
which may be a requirement for some organizations.
\paragraph{Containers}
Docker helps you build and deploy containers inside of which you can
package your applications and services. As we’ve just learned, containers
are launched from images and can contain one or more running processes.
You can think about images as the building or packing aspect of Docker and
the containers as the running or execution aspect of Docker.
A Docker container is:
An image format.
A set of standard operations.
An execution environment.
Docker borrows the concept of the standard shipping container, used to
transport goods globally, as a model for its containers. But instead of
shipping goods, Docker containers ship software.
Each container contains a software image – its ‘cargo’ – and, like its
physical counterpart, allows a set of operations to be performed. For
example, it can be created, started, stopped, restarted, and destroyed.
Like a shipping container, Docker doesn’t care about the contents of the
container when performing these actions; for example, whether a container
is a web server, a database, or an application server. Each container is
loaded the same as any other container.
Docker also doesn’t care where you ship your container: you can build on
your laptop, upload to a registry, then download to a physical or virtual
server, test, deploy to a cluster of a dozen Amazon EC2 hosts, and run. Like
a normal shipping container, it is interchangeable, stackable, portable, and
as generic as possible.
With Docker, we can quickly build an application server, a message bus, a
utility appliance, a CI test bed for an application, or one of a thousand other
possible applications, services, and tools. It can build local, self-contained
test environments or replicate complex application stacks for production or
development purposes. The possible use cases are endless.
   \section{Casos de estudio}
\subsection{Caso de estudio: Sniffing de la red para obtener credenciales}
(HAY MAS, CON GRAFICOS EN)
Joseph Muniz, Aamir Lakhani - Web Penetration Testing with Kali Linux-Packt Publishing (2013)
Aca yo segui un tutorial, buscarlo

La idea principal de esta sección es demostrar que, encontrandose en una red interna
y con con herramientas ya desarrolladas y libres es posible realizar un ataque 
sin necesidad de conocer a fondo la implementacion de la misma ni de tener mayores
privilegios

Recordar que esto fue realizado en una red interna donde son todos equipos de nuestra propiedad

\subsubsection{Diagrama de explicacion}
IMAGEN de le red

Tiene que estar:
-Router

-Origen de la pagina

-Consumidor de la pagina

-El atacante

\subsubsection{Preparando Ettercap para el ataque ARP Poisoning}

Lo primero que debemos hacer, en la lista de aplicaciones, es buscar el apartado 
«9. Sniffing y Spoofing«, ya que es allí donde encontraremos las herramientas necesarias
 para llevar a cabo este ataque.

IMAGEN Kali Linux Spoofing

A continuación, abriremos «Ettercap» y veremos una ventana similar a la siguiente.

IMAGEN  Kali Linux Ettercap

El siguiente paso es seleccionar la tarjeta de red con la que vamos a trabajar. Para ello, en el menú superior de Ettercap seleccionaremos «Sniff > Unified Sniffing» y, cuando nos lo pregunte, seleccionaremos nuestra tarjeta de red (por ejemplo, en nuestro caso, eth0).

IMAGEN Kali Linux - Ettercap - Tarjeta de red

El siguiente paso es buscar todos los hosts conectados a nuestra red local. Para ello, seleccionaremos «Hosts» del menú de la parte superior y seleccionaremos la primera opción, «Hosts List«.

IMAGEN Kali Linux - Ettercap - Lista de hosts

Aquí deberían salirnos todos los hosts o dispositivos conectados a nuestra red. Sin embargo, en caso de que no salgan todos, podemos realizar una exploración completa de la red simplemente abriendo el menú «Hosts» y seleccionando la opción «Scan for hosts«. Tras unos segundos, la lista de antes se debería actualizar mostrando todos los dispositivos, con sus respectivas IPs y MACs, conectados a nuestra red.

IMAGEN Kali Linux - Ettercap - Lista de hosts 2

\subsubsection{Nuestro Ettercap ya está listo. Ya podemos empezar con el ataque ARP Poisoning}

En caso de querer realizar un ataque dirigido contra un solo host, por ejemplo, suplantar la identidad de la puerta de enlace para monitorizar las conexiones del iPad que nos aparece en la lista de dispositivos, antes de empezar con el ataque debemos establecer los dos objetivos.

Para ello, debajo de la lista de hosts podemos ver tres botones, aunque nosotros prestaremos atención a los dos últimos:

    Target 1 – Seleccionamos la IP del dispositivo a monitorizar, en este caso, el iPad, y pulsamos sobre dicho botón.
    Target 2 – Pulsamos la IP que queremos suplantar, en este caso, la de la puerta de enlace.

IMAGEN Objetivos Ettercap

Todo listo. Ahora solo debemos elegir el menú «MITM» de la parte superior y, en él, escoger la opción «ARP Poisoning«.

IMAGEN Kali Linux - Ettercap - Ataques MITM

Nos aparecerá una pequeña ventana de configuración, en la cual debemos asegurarnos de marcar «Sniff Remote Connections«.

IMAGEN Comenzar MITM ARP Poisoning

Pulsamos sobre «Ok» y el ataque dará lugar. Ahora ya podemos tener el control sobre el host que hayamos establecido como «Target 1«. Lo siguiente que debemos hacer es, por ejemplo, ejecutar Wireshark para capturar todos los paquetes de red y analizarlos en busca de información interesante o recurrir a los diferentes plugins que nos ofrece Ettercap, como, por ejemplo, el navegador web remoto, donde nos cargará todas las webs que visite el objetivo.

Plugins Ettercap
